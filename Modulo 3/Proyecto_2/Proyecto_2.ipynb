{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Índice de Calidad del Aire</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.aire.cdmx.gob.mx/estadisticas-consultas/descargas/INDICExls-csv.pdf\n",
    "\n",
    "http://www.aire.cdmx.gob.mx/descargas/monitoreo/normatividad/NADF-009-AIRE-2017.pdf\n",
    "\n",
    "http://www.aire.cdmx.gob.mx/default.php?opc=%27aqBjnmU=%27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f # Elimina todas las variables del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install missingno \n",
    "# Libreria para la visualización de los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Seleccionar las variables de interes y unir los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define la ruta de la carpeta donde están los archivos CSV\n",
    "# La ruta es una cadena de texto cruda (raw string) que permite incluir barras invertidas sin que se interpreten como caracteres de escape\n",
    "carpeta = r'D:\\Diplomado-IA\\Modulo_3\\Proyectos\\Proyecto 1\\Original_dataset'\n",
    "\n",
    "# Lista para almacenar los DataFrames individuales\n",
    "# Aquí se va a ir guardando cada DataFrame que se genere a partir de los archivos CSV\n",
    "dataframes = []\n",
    "\n",
    "# Leer y almacenar las columnas relevantes de cada archivo CSV\n",
    "# Se recorre cada archivo en la carpeta especificada\n",
    "for archivo in os.listdir(carpeta):\n",
    "    # Verifica que el archivo termine en '.csv' para asegurarse de que estamos leyendo solo archivos CSV\n",
    "    if archivo.endswith('.csv'):\n",
    "        # Leer el archivo CSV\n",
    "        # Utiliza pandas para leer el archivo CSV y almacenarlo en un DataFrame\n",
    "        df = pd.read_csv(os.path.join(carpeta, archivo))\n",
    "        \n",
    "        # Seleccionar las columnas relevantes\n",
    "        # Se define una lista con las columnas que queremos mantener del DataFrame original\n",
    "        columnas_relevantes = ['Fecha', 'Hora']\n",
    "        # Añadimos las columnas específicas de cada zona que queremos conservar\n",
    "        for zona in ['Noroeste', 'Noreste', 'Centro', 'Suroeste', 'Sureste']:\n",
    "            columnas_relevantes.append(f'{zona} PM10')\n",
    "            columnas_relevantes.append(f'{zona} monóxido de carbono')\n",
    "        \n",
    "        # Filtrar el DataFrame para conservar solo las columnas relevantes\n",
    "        # Creamos un nuevo DataFrame que solo contiene las columnas especificadas en la lista 'columnas_relevantes'\n",
    "        df_relevante = df[columnas_relevantes]\n",
    "        \n",
    "        # Añadir el DataFrame filtrado a la lista\n",
    "        # Se agrega el DataFrame filtrado a la lista 'dataframes'\n",
    "        dataframes.append(df_relevante)\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "# Utilizamos pandas para concatenar todos los DataFrames de la lista 'dataframes' en un único DataFrame\n",
    "data_combinada = pd.concat(dataframes, ignore_index=True)\n",
    "# Concatenar por columnas result = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "# Guardamos el DataFrame combinado en un archivo CSV llamado 'data_combinada.csv'\n",
    "data_combinada.to_csv('data_combinada.csv', index=False)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame combinado\n",
    "# Mostramos las primeras filas del DataFrame combinado para verificar que se ha creado correctamente\n",
    "data_combinada.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import missingno as msno \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el DataFrame\n",
    "df = pd.read_csv('data_combinada.csv')\n",
    "df.describe().round() # Podemos redondear los valores de salida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar valores -99 con NaN\n",
    "df.replace(-99, np.nan, inplace=True)\n",
    "\n",
    "# Asegurarse de que a partir de la segunda columna todos los valores sean numéricos\n",
    "for col in df.columns[1:]:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Convertir la columna 'Fecha' al formato específico\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Verificar si hay fechas que no se pudieron convertir\n",
    "invalid_dates = df[df['Fecha'].isna()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Fechas no convertidas correctamente:\")\n",
    "    print(invalid_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Formatear la columna 'Fecha' al formato 'YYYY-MM-DD'\n",
    "df['Fecha'] = df['Fecha'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Renombrar columnas\n",
    "short_column_names = {\n",
    "    'Noroeste PM10': 'NO-PM10',\n",
    "    'Noroeste monóxido de carbono': 'NO-CO',\n",
    "    'Noreste PM10': 'NE-PM10',\n",
    "    'Noreste monóxido de carbono': 'NE-CO',\n",
    "    'Centro PM10': 'CE-PM10',\n",
    "    'Centro monóxido de carbono': 'CE-CO',\n",
    "    'Suroeste PM10': 'SO-PM10',\n",
    "    'Suroeste monóxido de carbono': 'SO-CO',\n",
    "    'Sureste PM10': 'SE-PM10',\n",
    "    'Sureste monóxido de carbono': 'SE-CO'\n",
    "}\n",
    "df.rename(columns=short_column_names, inplace=True)\n",
    "\n",
    "# Corregir las horas inválidas\n",
    "df['Hora'] = df['Hora'] - 1\n",
    "\n",
    "# Crear columna Fecha_Hora\n",
    "df['Fecha_Hora'] = pd.to_datetime(df['Fecha'] + ' ' + df['Hora'].astype(str) + ':00:00')\n",
    "df.set_index('Fecha_Hora', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df) # Visualización de los datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Descripción por año\n",
    "df['Año'] = df.index.year\n",
    "df['Mes'] = df.index.month\n",
    "\n",
    "# Calcular valores nulos por año\n",
    "nulos_por_año = df.isnull().groupby(df['Año']).sum()\n",
    "\n",
    "# Calcular valores nulos por mes para cada año\n",
    "nulos_por_mes = df.isnull().groupby([df['Año'], df['Mes']]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_por_año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_por_mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar series de tiempo con rango de fechas y zonas específicas\n",
    "def plot_time_series(df, variable, zones, ylabel, title, start_date=None, end_date=None):\n",
    "    # Configurar el tamaño de la figura del gráfico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Si se proporcionan fechas de inicio y fin, filtrar el DataFrame para ese rango de fechas\n",
    "    if start_date and end_date:\n",
    "        df = df.loc[start_date:end_date]\n",
    "    \n",
    "    # Iterar sobre cada zona especificada en la lista de zonas\n",
    "    for zone in zones:\n",
    "        # Crear el nombre de la columna combinando la zona y la variable\n",
    "        column = f\"{zone}-{variable}\"\n",
    "        \n",
    "        # Verificar si la columna existe en el DataFrame\n",
    "        if column in df.columns:\n",
    "            # Graficar la serie de tiempo de la columna\n",
    "            plt.plot(df.index, df[column], label=column)\n",
    "        else:\n",
    "            # Imprimir un mensaje si la columna no se encuentra en el DataFrame\n",
    "            print(f\"Columna {column} no encontrada en el DataFrame\")\n",
    "\n",
    "    # Etiqueta del eje x\n",
    "    plt.xlabel('Fecha_Hora')\n",
    "    # Etiqueta del eje y\n",
    "    plt.ylabel(ylabel)\n",
    "    # Título del gráfico\n",
    "    plt.title(title)\n",
    "    # Mostrar la leyenda del gráfico\n",
    "    plt.legend()\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()\n",
    "\n",
    "# Definir las zonas específicas que se desean graficar para PM10\n",
    "zones_to_plot = ['NO', 'NE', 'CE']\n",
    "\n",
    "# Llamar a la función para graficar PM10 en las zonas específicas con el rango de fechas indicado\n",
    "plot_time_series(df, 'PM10', zones_to_plot, 'Índice (PM10)', 'Series de Tiempo de PM10 en Diferentes Zonas', start_date='2016-01-01', end_date='2016-12-31')\n",
    "\n",
    "# Definir las zonas específicas que se desean graficar para CO\n",
    "zones_to_plot = ['NO', 'NE', 'CE']\n",
    "\n",
    "# Llamar a la función para graficar CO en las zonas específicas con el rango de fechas indicado\n",
    "plot_time_series(df, 'CO', zones_to_plot, 'Índice (Monóxido de Carbono)', 'Series de Tiempo de Monóxido de Carbono en Diferentes Zonas', start_date='2016-01-01', end_date='2016-12-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas para PM10 y CO\n",
    "pm10_columns = [col for col in df.columns if 'PM10' in col]\n",
    "co_columns = [col for col in df.columns if 'CO' in col]\n",
    "\n",
    "# Crear nuevas columnas para promedios\n",
    "df['MEAN-PM10'] = df[pm10_columns].mean(axis=1, skipna=True)\n",
    "df['MEAN-CO'] = df[co_columns].mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_time_series(df, 'PM10',['MEAN'], 'Índice (PM10)', 'MEDIA PM10', start_date='2016-01-01', end_date='2016-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_averages(df, start_date, end_date, columns, freq='D'):\n",
    "    \"\"\"\n",
    "    Calcula los promedios de las zonas para las columnas especificadas en un rango de fechas específico.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    freq (str): Frecuencia para agrupar los datos ('D' para día, 'W' para semana, 'M' para mes).\n",
    "    \n",
    "    Retorna:\n",
    "    DataFrame: Un DataFrame con los promedios calculados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear DataFrame vacío para almacenar los promedios\n",
    "    df_avg = pd.DataFrame()\n",
    "\n",
    "    # Agrupar por la frecuencia especificada y calcular los promedios\n",
    "    for col in columns:\n",
    "        # Filtrar las columnas del DataFrame que contienen el nombre de la variable especificada (col)\n",
    "        zone_columns = [c for c in df_filtered.columns if col in c]\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la media\n",
    "        df_avg[f'{col}'] = df_filtered[zone_columns].resample(freq).mean().mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Retornar el DataFrame con los promedios calculados\n",
    "    return df_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'CE-CO'\n",
    "df_avg = calculate_averages(df, start_date='2016-01-01', end_date='2016-12-31', columns=[col], freq='D')\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la media móvil (tendencia) con una ventana de 12 periodos para cada columna y agregarlo al DataFrame\n",
    "df_avg[f'{col}-trend'] = df_avg[col].rolling(window=12, min_periods=1).mean()\n",
    "\n",
    "# Configurar el tamaño de la figura del gráfico\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Graficar la serie de tiempo original de la columna\n",
    "plt.plot(df_avg.index, df_avg[col], label=col)\n",
    "\n",
    "# Graficar la serie de tiempo de la tendencia (media móvil) con una línea discontinua\n",
    "plt.plot(df_avg.index, df_avg[f'{col}-trend'], linestyle='--', label=f'{col} Trend')\n",
    "\n",
    "# Título del gráfico\n",
    "plt.title('Promedios por Periodo')\n",
    "\n",
    "# Etiqueta del eje x\n",
    "plt.xlabel('Fecha')\n",
    "\n",
    "# Etiqueta del eje y\n",
    "plt.ylabel('Promedio')\n",
    "\n",
    "# Mostrar la leyenda del gráfico\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular los promedios y desviaciones estándar\n",
    "def calculate_averages_and_std(df, start_date, end_date, columns, freq='D'):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    freq (str): Frecuencia para agrupar los datos ('D' para día, 'W' para semana, 'M' para mes).\n",
    "    \n",
    "    Retorna:\n",
    "    DataFrame: Un DataFrame con los promedios y desviaciones estándar calculados.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear DataFrames vacíos para almacenar los promedios y desviaciones estándar\n",
    "    df_avg = pd.DataFrame()\n",
    "    df_std = pd.DataFrame()\n",
    "\n",
    "    # Agrupar por la frecuencia especificada y calcular los promedios y desviaciones estándar\n",
    "    for col in columns:\n",
    "        # Filtrar las columnas del DataFrame que contienen el nombre de la variable especificada (col)\n",
    "        zone_columns = [c for c in df_filtered.columns if col in c]\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la media\n",
    "        df_avg[f'{col}'] = df_filtered[zone_columns].resample(freq).mean().mean(axis=1, skipna=True)\n",
    "        # Resamplear (reagrupar) los datos según la frecuencia especificada y calcular la desviación estándar\n",
    "        df_std[f'{col}'] = df_filtered[zone_columns].resample(freq).std().mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Retornar los DataFrames con los promedios y desviaciones estándar calculados\n",
    "    return df_avg, df_std\n",
    "\n",
    "# Definir la columna a analizar\n",
    "col = 'CE-CO'\n",
    "\n",
    "# Calcular los promedios y desviaciones estándar para la columna especificada en el rango de fechas y frecuencia dada\n",
    "df_avg, df_std = calculate_averages_and_std(df, start_date='2016-01-01', end_date='2018-12-31', columns=[col], freq='W')\n",
    "\n",
    "# Calcular la tendencia usando una media móvil con ventana de 5 periodos\n",
    "df_avg[f'{col}-trend'] = df_avg[col].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "# Configurar el tamaño de la figura del gráfico\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Graficar los promedios de la columna\n",
    "plt.plot(df_avg.index, df_avg[col], label=col)\n",
    "\n",
    "# Graficar la tendencia con una línea discontinua\n",
    "plt.plot(df_avg.index, df_avg[f'{col}-trend'], linestyle='--', label=f'{col} Trend')\n",
    "\n",
    "# Agregar banda de desviación estándar (área sombreada)\n",
    "plt.fill_between(df_avg.index, df_avg[col] - df_std[col], df_avg[col] + df_std[col], color='gray', alpha=0.2)\n",
    "\n",
    "# Título del gráfico\n",
    "plt.title('Promedios por Periodo con Desviación Estándar')\n",
    "\n",
    "# Etiqueta del eje x\n",
    "plt.xlabel('Fecha')\n",
    "\n",
    "# Etiqueta del eje y\n",
    "plt.ylabel('Promedio')\n",
    "\n",
    "# Mostrar la leyenda del gráfico\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomponer la serie temporal\n",
    "decomposition = seasonal_decompose(df_avg['CE-CO'], model='additive', period=12)\n",
    "\n",
    "# Graficar los componentes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(df_avg['CE-CO'], label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Serie Temporal Original')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(decomposition.trend, label='Tendencia')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente de Tendencia')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(decomposition.seasonal, label='Estacionalidad')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente de Estacionalidad')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(decomposition.resid, label='Residuales')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Componente Residual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hourly_boxplot(df, start_date, end_date, columns):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico por hora y genera una gráfica de cajas y bigotes.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    \n",
    "    Retorna:\n",
    "    None: Genera una gráfica de cajas y bigotes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "    # Crear un DataFrame solo con las columnas relevantes\n",
    "    df_hourly = df_filtered[['Hora'] + columns]\n",
    "\n",
    "    # Derretir el DataFrame para que sea apto para Seaborn\n",
    "    df_hourly_melted = df_hourly.melt(id_vars=['Hora'], value_vars=columns, var_name='Variable', value_name='Valor')\n",
    "\n",
    "    # Calcular la media por hora y variable\n",
    "    df_means = df_hourly.groupby('Hora')[columns].mean().reset_index()\n",
    "\n",
    "    # Graficar con Seaborn\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.boxplot(x='Hora', y='Valor', data=df_hourly_melted, showfliers=True)\n",
    "    \n",
    "    # Añadir la línea de media al gráfico\n",
    "    for col in columns:\n",
    "        sns.lineplot(x='Hora', y=col, data=df_means, label=f'Media {col}' , color = 'r')\n",
    "\n",
    "    plt.title('Concentración por hora')\n",
    "    plt.xlabel('Hora del día')\n",
    "    plt.ylabel('Concentración')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_hourly_boxplot(df, start_date='2016-01-01', end_date='2016-12-31', columns=['CE-CO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weekly_boxplot(df, start_date, end_date, columns):\n",
    "    \"\"\"\n",
    "    Calcula los promedios y desviaciones estándar de las zonas para las columnas especificadas en un rango de fechas específico por día de la semana y genera una gráfica de cajas y bigotes.\n",
    "    \n",
    "    Parámetros:\n",
    "    df (DataFrame): El DataFrame con los datos.\n",
    "    start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'.\n",
    "    end_date (str): Fecha de fin en formato 'YYYY-MM-DD'.\n",
    "    columns (list): Lista de columnas a promediar.\n",
    "    \n",
    "    Retorna:\n",
    "    None: Genera una gráfica de cajas y bigotes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas\n",
    "    df_filtered = df[(df.index >= start_date) & (df.index <= end_date)].copy()\n",
    "\n",
    "    # Agregar columna de día de la semana\n",
    "    df_filtered['Día de la semana'] = df_filtered.index.day_name()\n",
    "\n",
    "    # Crear un DataFrame solo con las columnas relevantes\n",
    "    df_weekly = df_filtered[['Día de la semana'] + columns]\n",
    "\n",
    "    # Derretir el DataFrame para que sea apto para Seaborn\n",
    "    df_weekly_melted = df_weekly.melt(id_vars=['Día de la semana'], value_vars=columns, var_name='Variable', value_name='Valor')\n",
    "\n",
    "    # Calcular la media por día de la semana y variable\n",
    "    df_means = df_weekly.groupby('Día de la semana')[columns].mean().reset_index()\n",
    "\n",
    "    # Ordenar los días de la semana\n",
    "    order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df_means['Día de la semana'] = pd.Categorical(df_means['Día de la semana'], categories=order, ordered=True)\n",
    "    df_means = df_means.sort_values('Día de la semana')\n",
    "\n",
    "    # Graficar con Seaborn\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.boxplot(x='Día de la semana', y='Valor', data=df_weekly_melted, order=order, showfliers=False)\n",
    "\n",
    "    # Añadir la línea de media al gráfico\n",
    "    for col in columns:\n",
    "        sns.lineplot(x='Día de la semana', y=col, data=df_means, label=f'Media {col}',color='r')\n",
    "\n",
    "    plt.title('Concentración por día de la semana')\n",
    "    plt.xlabel('Día de la semana')\n",
    "    plt.ylabel('Concentración')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_weekly_boxplot(df, start_date='2016-01-01', end_date='2016-12-31', columns=['CE-CO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los intervalos y colores según las tablas proporcionadas\n",
    "intervalos = [\n",
    "    (0, 50, '#9ACA3C'),     # Intervalo 0-50 con color verde\n",
    "    (51, 100, '#F7EC0F'),   # Intervalo 51-100 con color amarillo\n",
    "    (101, 150, '#F8991D'),  # Intervalo 101-150 con color naranja\n",
    "    (151, 200, '#ED2124'),  # Intervalo 151-200 con color rojo\n",
    "    (201, 300, '#7D287D'),  # Intervalo 201-300 con color morado\n",
    "    (301, 500, '#7E0023')   # Intervalo 301-500 con color marrón oscuro\n",
    "]\n",
    "\n",
    "# Función para obtener el color basado en el valor\n",
    "def obtener_color(valor):\n",
    "    for intervalo in intervalos:\n",
    "        if intervalo[0] <= valor <= intervalo[1]:\n",
    "            return intervalo[2]\n",
    "    return '#000000'  # Negro por defecto si el valor no encaja en ningún intervalo\n",
    "\n",
    "# Función para graficar series de tiempo con puntos coloreados según los intervalos\n",
    "def plot_time_series_points(df, variable, zones, ylabel, title, start_date=None, end_date=None):\n",
    "    # Configurar el tamaño de la figura del gráfico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Filtrar el DataFrame por el rango de fechas si se proporcionan fechas de inicio y fin\n",
    "    if start_date and end_date:\n",
    "        df = df.loc[start_date:end_date]\n",
    "    \n",
    "    # Iterar sobre cada zona especificada en la lista de zonas\n",
    "    for zone in zones:\n",
    "        # Crear el nombre de la columna combinando la zona y la variable\n",
    "        column = f\"{zone}-{variable}\"\n",
    "        \n",
    "        # Verificar si la columna existe en el DataFrame\n",
    "        if (column in df.columns):\n",
    "            # Aplicar la función obtener_color a cada valor de la columna para obtener los colores correspondientes\n",
    "            colors = df[column].apply(obtener_color)\n",
    "            # Graficar los puntos con los colores obtenidos\n",
    "            plt.scatter(df.index, df[column], c=colors, label=column)\n",
    "        else:\n",
    "            # Imprimir un mensaje si la columna no se encuentra en el DataFrame\n",
    "            print(f\"Columna {column} no encontrada en el DataFrame\")\n",
    "\n",
    "    # Etiqueta del eje x\n",
    "    plt.xlabel('Fecha_Hora')\n",
    "    # Etiqueta del eje y\n",
    "    plt.ylabel(ylabel)\n",
    "    # Título del gráfico\n",
    "    plt.title(title)\n",
    "    # Mostrar la leyenda del gráfico\n",
    "    plt.legend()\n",
    "    # Mostrar el gráfico\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_to_plot = ['CE']\n",
    "plot_time_series_points(df, 'PM10', zones_to_plot, 'Índice (PM10)', '', start_date='2016-01-01', end_date='2024-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputación de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot()\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Crear una instancia del imputador KNN con 12 vecinos\n",
    "imputer_knn = KNNImputer(n_neighbors=12)\n",
    "\n",
    "# Aplicar el imputador KNN para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = imputer_knn.fit_transform(df_impute)\n",
    "\n",
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la imputación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después de la imputación y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después de la imputación\n",
    "print(\"\\nNúmero de valores nulos después de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después de la imputación\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot()\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después de la Imputación)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Aplicar interpolación para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = df_impute.interpolate(method='time')\n",
    "\n",
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la interpolación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después de la interpolación y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después de la interpolación\n",
    "print(\"\\nNúmero de valores nulos después de la interpolación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después de la interpolación\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después de la Interpolación)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame para obtener las columnas 'CE-CO' y 'CE-PM10' en el rango de fechas especificado\n",
    "df_impute = df[['CE-CO', 'CE-PM10']].loc['2016-01-01':'2019-05-01']\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame filtrado\n",
    "print(\"Primeras filas del DataFrame filtrado:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10'\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute.plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "# Contar y mostrar el número de valores nulos en cada columna del DataFrame filtrado\n",
    "print(\"\\nNúmero de valores nulos antes de la imputación:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Aplicar el promedio móvil para llenar los valores nulos en el DataFrame\n",
    "df_impute.loc[:, :] = df_impute.fillna(df_impute.rolling(window=12, min_periods=1).mean())\n",
    "\n",
    "# Agregar una columna con el día juliano (día del año)\n",
    "df_impute['julian_day'] = df_impute.index.dayofyear\n",
    "\n",
    "# Agregar una columna con la hora del día\n",
    "df_impute['hour'] = df_impute.index.hour\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame después de la imputación y adición de columnas\n",
    "print(\"\\nPrimeras filas del DataFrame después del promedio móvil y adición de columnas:\")\n",
    "print(df_impute.head())\n",
    "\n",
    "# Contar y mostrar el número de valores nulos después del promedio móvil\n",
    "print(\"\\nNúmero de valores nulos después del promedio móvil:\")\n",
    "print(df_impute.isnull().sum())\n",
    "\n",
    "# Graficar las series de tiempo de 'CE-CO' y 'CE-PM10' después del promedio móvil\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_impute[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después del Promedio Móvil)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Filtrado de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Copiar el DataFrame para aplicar diferentes filtros\n",
    "df_filtered_savgol = df_impute.copy()\n",
    "\n",
    "\n",
    "# Aplicar el filtro Savitzky-Golay para suavizar los datos\n",
    "window_length = 13  # El tamaño de la ventana debe ser un número impar\n",
    "polyorder = 3       # El orden del polinomio\n",
    "df_filtered_savgol['CE-CO'] = savgol_filter(df_filtered_savgol['CE-CO'], window_length=window_length, polyorder=polyorder)\n",
    "df_filtered_savgol['CE-PM10'] = savgol_filter(df_filtered_savgol['CE-PM10'], window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "\n",
    "# Graficar las series de tiempo después de aplicar el filtro Savitzky-Golay\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_filtered_savgol[['CE-CO', 'CE-PM10']].plot(ax=plt.gca())\n",
    "plt.title('Series de Tiempo de CE-CO y CE-PM10 (Después del Filtro Savitzky-Golay)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Concentración')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_savgol.to_csv('Datos_procesados.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático medio (RMSE) entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Valores verdaderos.\n",
    "        y_pred (tensor): Valores predichos.\n",
    "\n",
    "    Returns:\n",
    "        tensor: RMSE.\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def split_dataframe(df, train_ratio=0.7, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en conjuntos de entrenamiento, prueba y evaluación.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame a dividir.\n",
    "        train_ratio (float, optional): Proporción de datos para entrenamiento. Defaults to 0.7.\n",
    "        test_ratio (float, optional): Proporción de datos para prueba. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataFrames de entrenamiento, prueba y evaluación.\n",
    "    \"\"\"\n",
    "    total_size = len(df)  # Tamaño total del DataFrame\n",
    "    train_size = int(total_size * train_ratio)  # Tamaño del conjunto de entrenamiento\n",
    "    test_size = int(total_size * test_ratio)  # Tamaño del conjunto de prueba\n",
    "    eval_size = total_size - train_size - test_size  # Tamaño del conjunto de evaluación\n",
    "    \n",
    "    df_train = df.iloc[:train_size]  # Conjunto de entrenamiento\n",
    "    df_test = df.iloc[train_size:train_size + test_size]  # Conjunto de prueba\n",
    "    df_eval = df.iloc[train_size + test_size:]  # Conjunto de evaluación\n",
    "    \n",
    "    return df_train, df_test, df_eval\n",
    "\n",
    "\n",
    "def scale_datasets(train_df, test_df, eval_df):\n",
    "    \"\"\"\n",
    "    Escala los conjuntos de datos de entrenamiento, prueba y evaluación.\n",
    "\n",
    "    Args:\n",
    "        train_df (DataFrame): DataFrame de entrenamiento.\n",
    "        test_df (DataFrame): DataFrame de prueba.\n",
    "        eval_df (DataFrame): DataFrame de evaluación.\n",
    "\n",
    "    Returns:\n",
    "        tuple: DataFrames escalados de entrenamiento, prueba y evaluación, y diccionario de escaladores.\n",
    "    \"\"\"\n",
    "    scalers = {}  # Diccionario para almacenar los escaladores\n",
    "    scaled_train_df = train_df.copy()  # Copia del DataFrame de entrenamiento\n",
    "    scaled_test_df = test_df.copy()  # Copia del DataFrame de prueba\n",
    "    scaled_eval_df = eval_df.copy()  # Copia del DataFrame de evaluación\n",
    "    \n",
    "    for column in train_df.columns:\n",
    "        scaler = MinMaxScaler()  # Crear un escalador MinMax para cada columna\n",
    "        scaled_train_df[column] = scaler.fit_transform(train_df[[column]])  # Ajustar y transformar los datos de entrenamiento\n",
    "        scaled_test_df[column] = scaler.transform(test_df[[column]])  # Transformar los datos de prueba\n",
    "        scaled_eval_df[column] = scaler.transform(eval_df[[column]])  # Transformar los datos de evaluación\n",
    "        scalers[column] = scaler  # Guardar el escalador para cada columna\n",
    "        \n",
    "    return scaled_train_df, scaled_test_df, scaled_eval_df, scalers\n",
    "\n",
    "def create_sequences(data, input_columns, output_columns, time_steps, future_steps=1):\n",
    "    \"\"\"\n",
    "    Crea secuencias de datos para modelado de series temporales.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame con los datos.\n",
    "        input_columns (list): Columnas de entrada.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias de entrada.\n",
    "        future_steps (int, optional): Número de pasos de tiempo futuros para las secuencias de salida. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Arrays de entrada y salida para el modelo.\n",
    "    \"\"\"\n",
    "    X, y = [], []  # Listas para almacenar las secuencias de entrada y salida\n",
    "    data_array = data.values  # Convertir el DataFrame a un array numpy\n",
    "\n",
    "    input_indices = data.columns.get_indexer(input_columns)  # Índices de las columnas de entrada\n",
    "    output_indices = data.columns.get_indexer(output_columns)  # Índices de las columnas de salida\n",
    "\n",
    "    for i in range(len(data) - time_steps - future_steps + 1):\n",
    "        X.append(data_array[i:i + time_steps, input_indices])  # Crear secuencia de entrada\n",
    "        y.append(data_array[i + time_steps:i + time_steps + future_steps, output_indices].flatten())  # Crear secuencia de salida\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_simple_rnn_model(input_shape, output_size, future_steps, rnn_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo SimpleRNN.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        rnn_units (int, optional): Unidades de la capa RNN. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de SimpleRNN.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(SimpleRNN(rnn_units, input_shape=input_shape, return_sequences=False))  # Añadir capa SimpleRNN\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape, output_size, future_steps, lstm_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo LSTM.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        lstm_units (int, optional): Unidades de la capa LSTM. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de LSTM.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(LSTM(lstm_units, input_shape=input_shape, return_sequences=False))  # Añadir capa LSTM\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_gru_model(input_shape, output_size, future_steps, gru_units=50, hidden_units=50, output_activation='linear'):\n",
    "    \"\"\"\n",
    "    Crea un modelo GRU.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Forma de las secuencias de entrada.\n",
    "        output_size (int): Tamaño de la salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        gru_units (int, optional): Unidades de la capa GRU. Defaults to 50.\n",
    "        hidden_units (int, optional): Unidades de la capa oculta. Defaults to 50.\n",
    "        output_activation (str, optional): Activación de la capa de salida. Defaults to 'linear'.\n",
    "\n",
    "    Returns:\n",
    "        model: Modelo compilado de GRU.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Crear un modelo secuencial\n",
    "    model.add(GRU(gru_units, input_shape=input_shape, return_sequences=False))  # Añadir capa GRU\n",
    "    model.add(Dense(hidden_units, activation='relu'))  # Añadir capa oculta densa\n",
    "    model.add(Dense(output_size * future_steps, activation=output_activation))  # Añadir capa de salida\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse])  # Compilar el modelo\n",
    "    return model\n",
    "\n",
    "def plot_final_prediction(model, X, y, scalers, output_columns, future_steps, title):\n",
    "    \"\"\"\n",
    "    Grafica las predicciones finales del modelo en comparación con los valores reales.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): Modelo entrenado.\n",
    "        X (ndarray): Datos de entrada.\n",
    "        y (ndarray): Valores reales.\n",
    "        scalers (dict): Diccionario de escaladores para reescalar los datos.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        title (str): Título de la gráfica.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X).reshape(-1, future_steps, len(output_columns))  # Realizar predicciones y darles forma\n",
    "    \n",
    "    # Reescalar las predicciones y los valores reales\n",
    "    y_rescaled = np.zeros((y.shape[0], len(output_columns)))\n",
    "    predictions_rescaled = np.zeros((predictions.shape[0], future_steps, len(output_columns)))\n",
    "    \n",
    "    for i, column in enumerate(output_columns):\n",
    "        y_rescaled[:, i] = scalers[column].inverse_transform(y[:, i].reshape(-1, 1)).reshape(-1)  # Reescalar valores reales\n",
    "        for step in range(future_steps):\n",
    "            predictions_rescaled[:, step, i] = scalers[column].inverse_transform(predictions[:, step, i].reshape(-1, 1)).reshape(-1)  # Reescalar predicciones\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))  # Crear figura\n",
    "    for i in range(len(output_columns)):\n",
    "        plt.plot(y_rescaled[:, i], label='Valor Real')  # Graficar valores reales\n",
    "        plt.plot(predictions_rescaled[:, -1, i], label='Predicción')  # Graficar predicciones\n",
    "        plt.title(f'{title} - Variable {output_columns[i]}')  # Añadir título\n",
    "        plt.legend()  \n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show() \n",
    "\n",
    "# Función para graficar los errores finales del modelo\n",
    "def plot_final_errors(model, X, y, scalers, output_columns, future_steps, title):\n",
    "    \"\"\"\n",
    "    Grafica los errores finales del modelo en comparación con los valores reales.\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): Modelo entrenado.\n",
    "        X (ndarray): Datos de entrada.\n",
    "        y (ndarray): Valores reales.\n",
    "        scalers (dict): Diccionario de escaladores para reescalar los datos.\n",
    "        output_columns (list): Columnas de salida.\n",
    "        future_steps (int): Número de pasos de tiempo futuros.\n",
    "        title (str): Título de la gráfica.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X).reshape(-1, future_steps, len(output_columns))  # Realizar predicciones y darles forma\n",
    "    \n",
    "    # Reescalar las predicciones y los valores reales\n",
    "    y_rescaled = np.zeros((y.shape[0], len(output_columns)))\n",
    "    predictions_rescaled = np.zeros((predictions.shape[0], future_steps, len(output_columns)))\n",
    "    \n",
    "    for i, column in enumerate(output_columns):\n",
    "        y_rescaled[:, i] = scalers[column].inverse_transform(y[:, i].reshape(-1, 1)).reshape(-1)  # Reescalar valores reales\n",
    "        for step in range(future_steps):\n",
    "            predictions_rescaled[:, step, i] = scalers[column].inverse_transform(predictions[:, step, i].reshape(-1, 1)).reshape(-1)  # Reescalar predicciones\n",
    "    \n",
    "    errors_rescaled = y_rescaled - predictions_rescaled[:, -1, :]  # Calcular errores reescalados\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))  # Crear figura\n",
    "    for i in range(len(output_columns)):\n",
    "        plt.plot(errors_rescaled[:, i], label='Error')  # Graficar errores\n",
    "        plt.title(f'{title} - Error Variable {output_columns[i]}')  # Añadir título\n",
    "        plt.legend() \n",
    "    \n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history, title='Historial de Entrenamiento'):\n",
    "    \"\"\"\n",
    "    Grafica el historial de entrenamiento del modelo.\n",
    "\n",
    "    Args:\n",
    "        history (History): Historial de entrenamiento del modelo.\n",
    "        title (str, optional): Título de la gráfica. Defaults to 'Historial de Entrenamiento'.\n",
    "    \"\"\"\n",
    "    # Extraer la información del historial\n",
    "    history_dict = history.history\n",
    "    \n",
    "    # Crear una figura\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Graficar la pérdida (loss)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_dict['loss'], label='Pérdida en Entrenamiento')\n",
    "    if 'val_loss' in history_dict:\n",
    "        plt.plot(history_dict['val_loss'], label='Pérdida en Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.title('Pérdida durante el Entrenamiento')\n",
    "    plt.legend()\n",
    "\n",
    "    # Graficar la métrica RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['rmse'], label='RMSE en Entrenamiento')\n",
    "    if 'val_rmse' in history_dict:\n",
    "        plt.plot(history_dict['val_rmse'], label='RMSE en Validación')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('RMSE durante el Entrenamiento')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ajustar el diseño y mostrar la gráfica\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Leer el archivo CSV\n",
    "file_path = 'Datos_procesados.csv'  # Reemplaza con la ruta a tu archivo\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df_train, df_test, df_eval = split_dataframe(df, train_ratio=0.7, test_ratio=0.2)\n",
    "\n",
    "print(f\"Tamaño de datos de entrenamiento: {len(df_train)}\")\n",
    "print(f\"Tamaño de datos de prueba: {len(df_test)}\")\n",
    "print(f\"Tamaño de datos de evaluación: {len(df_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_df, scaled_test_df, scaled_eval_df, scalers = scale_datasets(df_train, df_test, df_eval)\n",
    "\n",
    "time_steps = 24\n",
    "future_steps = 12\n",
    "#input_columns = ['CE-CO','CE-PM10','julian_day','hour']  # Nombres de las columnas de entrada en el DataFrame escalado\n",
    "input_columns = ['CE-CO']  # Nombres de las columnas de entrada en el DataFrame escalado\n",
    "output_columns = ['CE-CO']  # Nombres de las columnas de salida en el DataFrame escalado\n",
    "\n",
    "\n",
    "X_train, y_train = create_sequences(scaled_train_df, input_columns, output_columns, time_steps, future_steps)\n",
    "X_test, y_test = create_sequences(scaled_test_df, input_columns, output_columns, time_steps, future_steps)\n",
    "X_eval, y_eval = create_sequences(scaled_eval_df, input_columns, output_columns, time_steps, future_steps)\n",
    "\n",
    "print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}\")\n",
    "print(f\"Tamaño de X_test: {X_test.shape}\")\n",
    "print(f\"Tamaño de y_test: {y_test.shape}\")\n",
    "print(f\"Tamaño de X_eval: {X_eval.shape}\")\n",
    "print(f\"Tamaño de y_eval: {y_eval.shape}\")\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "output_size = len(output_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo SimpleRNN\n",
    "model_simple_rnn = create_simple_rnn_model(input_shape, output_size, future_steps)\n",
    "history_simple_rnn = model_simple_rnn.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_simple_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo LSTM\n",
    "model_lstm = create_lstm_model(input_shape, output_size, future_steps)\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el modelo GRU\n",
    "model_gru = create_gru_model(input_shape, output_size, future_steps)\n",
    "history_gru = model_gru.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test), shuffle=True)\n",
    "plot_training_history(history_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar los modelos en el conjunto de evaluación\n",
    "loss_simple_rnn, rmse_simple_rnn = model_simple_rnn.evaluate(X_eval, y_eval)\n",
    "print(f\"SimpleRNN - Loss en el conjunto de evaluación: {loss_simple_rnn}\")\n",
    "print(f\"SimpleRNN - RMSE en el conjunto de evaluación: {rmse_simple_rnn}\")\n",
    "\n",
    "loss_lstm, rmse_lstm = model_lstm.evaluate(X_eval, y_eval)\n",
    "print(f\"LSTM - Loss en el conjunto de evaluación: {loss_lstm}\")\n",
    "print(f\"LSTM - RMSE en el conjunto de evaluación: {rmse_lstm}\")\n",
    "\n",
    "loss_gru, rmse_gru = model_gru.evaluate(X_eval, y_eval)\n",
    "print(f\"GRU - Loss en el conjunto de evaluación: {loss_gru}\")\n",
    "print(f\"GRU - RMSE en el conjunto de evaluación: {rmse_gru}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_prediction(model_simple_rnn, X_eval, y_eval, scalers, output_columns, future_steps, title='SimpleRNN - Predicción Final')\n",
    "plot_final_errors(model_simple_rnn, X_eval, y_eval, scalers, output_columns, future_steps, title='SimpleRNN - Error Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones y errores para el modelo LSTM\n",
    "plot_final_prediction(model_lstm, X_eval, y_eval, scalers, output_columns, future_steps, title='LSTM - Predicciones en el conjunto de evaluación')\n",
    "plot_final_errors(model_lstm, X_eval, y_eval, scalers, output_columns, future_steps, title='LSTM - Errores en el conjunto de evaluación')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualizar predicciones y errores para el modelo GRU\n",
    "plot_final_prediction(model_gru, X_eval, y_eval, scalers, output_columns, future_steps, title='GRU - Predicciones en el conjunto de evaluación')\n",
    "plot_final_errors(model_gru, X_eval, y_eval, scalers, output_columns, future_steps, title='GRU - Errores en el conjunto de evaluación')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(model, data, input_columns, output_columns, time_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Añade múltiples predicciones al DataFrame de una sola vez.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado.\n",
    "        data (pd.DataFrame): DataFrame con los datos originales.\n",
    "        input_columns (list of str): Lista de nombres de las columnas de entrada.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las predicciones añadidas.\n",
    "    \"\"\"\n",
    "    # Escaladores\n",
    "    scalers = {col: MinMaxScaler() for col in data.columns}\n",
    "    \n",
    "    # Escalar datos\n",
    "    scaled_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        scaled_data[col] = scalers[col].fit_transform(data[[col]])\n",
    "    \n",
    "    # Seleccionar la última secuencia de time_steps\n",
    "    last_seq = scaled_data[input_columns].values[-time_steps:]\n",
    "    last_seq = np.expand_dims(last_seq, axis=0)\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    predictions_scaled = model.predict(last_seq)\n",
    "    \n",
    "    # Reescalar las predicciones\n",
    "    predictions = np.zeros((future_steps, len(output_columns)))\n",
    "    for i, col in enumerate(output_columns):\n",
    "        predictions[:, i] = scalers[col].inverse_transform(predictions_scaled[:, i::len(output_columns)].reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # Crear un DataFrame para las predicciones\n",
    "    predictions_df = pd.DataFrame(predictions, columns=output_columns)\n",
    "    \n",
    "    # Añadir las predicciones al DataFrame original\n",
    "    data_with_predictions = pd.concat([data, predictions_df], ignore_index=True)\n",
    "    \n",
    "    return data_with_predictions\n",
    "\n",
    "def plot_with_predictions(data, output_columns, plot_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Visualiza los datos actuales y las predicciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos actuales y las predicciones.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        plot_steps (int): Número de pasos de tiempo a graficar antes de la predicción.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for col in output_columns:\n",
    "        plt.plot(data.index[-(plot_steps + future_steps):], data[col].iloc[-(plot_steps + future_steps):], label=f'Actual {col}')\n",
    "        plt.plot(data.index[-future_steps:], data[col].iloc[-future_steps:], 'ro-', label=f'Predicción {col}')\n",
    "    \n",
    "    plt.title('Predicciones y datos actuales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_steps = 12  # Número de pasos de tiempo a graficar antes de la predicción\n",
    "\n",
    "# Añadir predicciones y visualizar\n",
    "df_with_predictions = add_predictions(model_simple_rnn, df, input_columns, output_columns, time_steps, future_steps)\n",
    "plot_with_predictions(df_with_predictions, output_columns, plot_steps, future_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn.save('model_rnn.keras')\n",
    "model_gru.save('model_gru.keras')\n",
    "model_lstm.save('model_lstm.keras')\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def save_scalers(scalers, directory):\n",
    "    \"\"\"\n",
    "    Guarda los escaladores en archivos separados.\n",
    "\n",
    "    Args:\n",
    "        scalers (dict): Diccionario de escaladores.\n",
    "        directory (str): Directorio donde se guardarán los escaladores.\n",
    "    \"\"\"\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Guardar cada escalador en un archivo separado\n",
    "    for column, scaler in scalers.items():\n",
    "        file_path = os.path.join(directory, f'{column}_scaler.pkl')\n",
    "        joblib.dump(scaler, file_path)\n",
    "        print(f\"Escalador para {column} guardado en {file_path}\")\n",
    "\n",
    "\n",
    "save_scalers(scalers, 'scalers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "file_path = 'Datos_procesados.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Definición de funciones\n",
    "\n",
    "def load_scalers(directory, columns):\n",
    "    \"\"\"\n",
    "    Carga los escaladores desde archivos separados.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directorio desde donde se cargarán los escaladores.\n",
    "        columns (list): Lista de nombres de columnas para las cuales se cargan los escaladores.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario de escaladores cargados.\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    \n",
    "    # Cargar cada escalador desde su archivo\n",
    "    for column in columns:\n",
    "        file_path = os.path.join(directory, f'{column}_scaler.pkl')\n",
    "        if os.path.exists(file_path):\n",
    "            scalers[column] = joblib.load(file_path)\n",
    "            print(f\"Escalador para {column} cargado desde {file_path}\")\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo {file_path} para {column}\")\n",
    "    \n",
    "    return scalers\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático medio (RMSE) entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "        y_true (tensor): Valores verdaderos.\n",
    "        y_pred (tensor): Valores predichos.\n",
    "\n",
    "    Returns:\n",
    "        tensor: RMSE.\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def add_predictions(model, data, input_columns, output_columns, time_steps, future_steps, scalers):\n",
    "    \"\"\"\n",
    "    Añade múltiples predicciones al DataFrame de una sola vez.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado.\n",
    "        data (pd.DataFrame): DataFrame con los datos originales.\n",
    "        input_columns (list of str): Lista de nombres de las columnas de entrada.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        time_steps (int): Número de pasos de tiempo para las secuencias.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "        scalers (dict): Diccionario de escaladores.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con las predicciones añadidas.\n",
    "    \"\"\"\n",
    "    # Escalar datos\n",
    "    scaled_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        scaled_data[col] = scalers[col].transform(data[[col]])\n",
    "    \n",
    "    # Seleccionar la última secuencia de time_steps\n",
    "    last_seq = scaled_data[input_columns].values[-time_steps:]\n",
    "    last_seq = np.expand_dims(last_seq, axis=0)\n",
    "    \n",
    "    # Realizar la predicción\n",
    "    predictions_scaled = model.predict(last_seq)\n",
    "    \n",
    "    # Reescalar las predicciones\n",
    "    predictions = np.zeros((future_steps, len(output_columns)))\n",
    "    for i, col in enumerate(output_columns):\n",
    "        predictions[:, i] = scalers[col].inverse_transform(predictions_scaled[:, i::len(output_columns)].reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # Crear un DataFrame para las predicciones\n",
    "    predictions_df = pd.DataFrame(predictions, columns=output_columns)\n",
    "    \n",
    "    # Añadir las predicciones al DataFrame original\n",
    "    data_with_predictions = pd.concat([data, predictions_df], ignore_index=True)\n",
    "    \n",
    "    return data_with_predictions\n",
    "\n",
    "def plot_with_predictions(data, output_columns, plot_steps, future_steps):\n",
    "    \"\"\"\n",
    "    Visualiza los datos actuales y las predicciones.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame con los datos actuales y las predicciones.\n",
    "        output_columns (list of str): Lista de nombres de las columnas de salida.\n",
    "        plot_steps (int): Número de pasos de tiempo a graficar antes de la predicción.\n",
    "        future_steps (int): Número de pasos de tiempo futuros a predecir.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    for col in output_columns:\n",
    "        plt.plot(data.index[-(plot_steps + future_steps):], data[col].iloc[-(plot_steps + future_steps):], label=f'Actual {col}')\n",
    "        plt.plot(data.index[-future_steps:], data[col].iloc[-future_steps:], 'ro-', label=f'Predicción {col}')\n",
    "    \n",
    "    plt.title('Predicciones y datos actuales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Cargar los escaladores\n",
    "columns = df.columns\n",
    "loaded_scalers = load_scalers('scalers', columns)\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = tf.keras.models.load_model('model_rnn.keras', custom_objects={'rmse': rmse})\n",
    "\n",
    "# Definir parámetros\n",
    "time_steps = 24\n",
    "future_steps = 12\n",
    "input_columns = ['CE-CO', 'CE-PM10', 'julian_day', 'hour']\n",
    "output_columns = ['CE-CO']\n",
    "plot_steps = 12  # Número de pasos de tiempo a graficar antes de la predicción\n",
    "\n",
    "# Añadir predicciones y visualizar\n",
    "df_with_predictions = add_predictions(model, df, input_columns, output_columns, time_steps, future_steps, loaded_scalers)\n",
    "plot_with_predictions(df_with_predictions, output_columns, plot_steps, future_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En desarrollo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_nowcast(data, window=12):\n",
    "    \"\"\"\n",
    "    Calcula el nowcast basado en una ventana móvil sobre los datos de concentración.\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): Serie temporal de datos de concentración.\n",
    "        window (int, optional): Tamaño de la ventana móvil. Por defecto es 12.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Arreglo con los valores de nowcast calculados.\n",
    "    \"\"\"\n",
    "    # Inicializar el arreglo de nowcast con NaN\n",
    "    nowcast = np.full(len(data), np.nan, dtype=np.float16)\n",
    "    \n",
    "    # Iterar sobre los datos comenzando desde el índice 'window - 1'\n",
    "    for i in range(window - 1, len(data)):\n",
    "        # Obtener los datos de la ventana actual\n",
    "        window_data = data.iloc[i - window + 1:i + 1].to_numpy()\n",
    "        \n",
    "        # Calcular el valor máximo y mínimo de la ventana\n",
    "        C_max = np.nanmax(window_data)\n",
    "        C_min = np.nanmin(window_data)\n",
    "        \n",
    "        # Calcular el peso 'w' basado en la diferencia entre el máximo y el mínimo\n",
    "        w = 1 - (C_max - C_min) / C_max if C_max != 0 else 0\n",
    "        \n",
    "        # Ajustar el peso 'W' para que no sea menor a 0.5\n",
    "        W = w if w > 0.5 else 0.5\n",
    "        \n",
    "        # Calcular los pesos para cada valor en la ventana\n",
    "        weights = np.where(~np.isnan(window_data), W ** np.arange(window), 0)\n",
    "        \n",
    "        # Sumar los pesos, ignorando los NaN\n",
    "        weights_sum = np.nansum(weights)\n",
    "        \n",
    "        # Si en los últimos tres valores de la ventana hay como máximo dos NaN\n",
    "        if np.count_nonzero(np.isnan(window_data[-3:])) <= 2:\n",
    "            # Calcular el nowcast como la media ponderada de los valores en la ventana\n",
    "            nowcast[i] = np.round(np.nansum(window_data * weights) / weights_sum)\n",
    "    \n",
    "    return nowcast\n",
    "\n",
    "# Lista de concentraciones\n",
    "concentraciones = [13, np.nan, 10, 21, 74, 64, 53, 82, 90, 75, 80, 50]\n",
    "\n",
    "# Convertir la lista en una Serie de pandas\n",
    "concentraciones_series = pd.Series(concentraciones)\n",
    "\n",
    "# Aplicar la función calculate_nowcast\n",
    "nowcast_result = calculate_nowcast(concentraciones_series)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(nowcast_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear un DataFrame ficticio\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Fecha': pd.date_range(start='2022-01-01', periods=100, freq='D'),\n",
    "    'Variable1': np.random.randn(100),\n",
    "    'Variable2': np.random.randn(100),\n",
    "    'Variable3': np.random.randn(100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducir valores faltantes\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable1'] = np.nan\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable2'] = np.nan\n",
    "df.loc[df.sample(frac=0.1).index, 'Variable3'] = np.nan\n",
    "\n",
    "print(df.head(10))\n",
    "df_media = df.copy()\n",
    "df_media['Variable1'] = df_media['Variable1'].fillna(df_media['Variable1'].mean())\n",
    "df_media['Variable2'] = df_media['Variable2'].fillna(df_media['Variable2'].mean())\n",
    "df_media['Variable3'] = df_media['Variable3'].fillna(df_media['Variable3'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df_knn = df.copy()\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "df_knn.iloc[:, 1:] = imputer_knn.fit_transform(df_knn.iloc[:, 1:])\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df_mice = df.copy()\n",
    "imputer_mice = IterativeImputer()\n",
    "df_mice.iloc[:, 1:] = imputer_mice.fit_transform(df_mice.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_statistics(df, method):\n",
    "    summary = df.describe().T[['mean', 'std']]\n",
    "    summary['method'] = method\n",
    "    return summary\n",
    "\n",
    "summary_original = summary_statistics(df.dropna(), 'Original')\n",
    "summary_media = summary_statistics(df_media, 'Media')\n",
    "summary_knn = summary_statistics(df_knn, 'KNN')\n",
    "summary_mice = summary_statistics(df_mice, 'MICE')\n",
    "\n",
    "summary_df = pd.concat([summary_original, summary_media, summary_knn, summary_mice])\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
